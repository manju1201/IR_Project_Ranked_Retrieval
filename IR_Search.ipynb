{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_Search.ipynb",
      "provenance": [],
      "mount_file_id": "15vXmM1MZiBM-sp8YZ9a20leqoeAi_sAQ",
      "authorship_tag": "ABX9TyNDM29fXcHbBmF4AtBWKQIU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manju1201/IR_Project_Ranked_Retrieval/blob/main/IR_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6Uy3KPHg7dD"
      },
      "source": [
        "**Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUoKdE7JhUUj",
        "outputId": "56296de5-7514-4aa6-f56a-ab9042ed746a"
      },
      "source": [
        "pip install wordninja"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wordninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/15/abe4af50f4be92b60c25e43c1c64d08453b51e46c32981d80b3aebec0260/wordninja-2.0.0.tar.gz (541kB)\n",
            "\r\u001b[K     |▋                               | 10kB 24.6MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 17.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 15.2MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 15.1MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 11.5MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61kB 11.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 71kB 11.6MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 92kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 102kB 12.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 112kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 122kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133kB 12.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 143kB 12.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153kB 12.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 163kB 12.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 174kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 184kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 194kB 12.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 204kB 12.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 215kB 12.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 225kB 12.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 235kB 12.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 245kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 256kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 266kB 12.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 276kB 12.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 286kB 12.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 296kB 12.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 307kB 12.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 317kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 327kB 12.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 337kB 12.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 348kB 12.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 358kB 12.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 368kB 12.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 378kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 389kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 399kB 12.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 409kB 12.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 419kB 12.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 430kB 12.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 440kB 12.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 450kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 460kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 471kB 12.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 481kB 12.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 491kB 12.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 501kB 12.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 512kB 12.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 522kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 532kB 12.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542kB 12.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: wordninja\n",
            "  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordninja: filename=wordninja-2.0.0-cp36-none-any.whl size=541553 sha256=b0ffe3a9be8559e8d31fb4ced9cc562e7a5418cc05a5d915257a7987287f0d39\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/46/06/9b6d10ed02c85e93c3bb33ac50e2d368b2586248f192a2e22a\n",
            "Successfully built wordninja\n",
            "Installing collected packages: wordninja\n",
            "Successfully installed wordninja-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6xSeaK-O6R0"
      },
      "source": [
        "import nltk\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import RegexpTokenizer\r\n",
        "import wordninja"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InAZrzjBPe4q",
        "outputId": "00733714-3dd3-4e70-b373-a02292fd9b78"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGiBaL6ShOvq"
      },
      "source": [
        "Removes Html codes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEA05n8DRdVW"
      },
      "source": [
        "def remove_htmlcodes(document):\r\n",
        "    replacement = {\r\n",
        "                    \"&ampnbsp\": ' ',\r\n",
        "                    \"&ampamp\": '&',\r\n",
        "                    \"&ampquot\": '\\'',\r\n",
        "                    \"&ampldquo\": '\\\"',\r\n",
        "                    \"&amprdquo\": '\\\"',\r\n",
        "                    \"&amplsquo\": '\\'',\r\n",
        "                    \"&amprsquo\": '\\'',\r\n",
        "                    \"&amphellip\": '...',\r\n",
        "                    \"&ampndash\": '-',\r\n",
        "                    \"&ampmdash\": '-'\r\n",
        "                  }\r\n",
        "    for str in replacement:\r\n",
        "        document = document.replace(str, replacement[str])\r\n",
        "        \r\n",
        "    return document"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RLAqbZThiCn"
      },
      "source": [
        "**Preprocessing**\r\n",
        "\r\n",
        "*   Parts of Speech tags,\r\n",
        "*   lemmatization,\r\n",
        "*   Tokenize, \r\n",
        "*   remove stopwords\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60C6tDi5PkhX"
      },
      "source": [
        "def get_wordnet_pos(word):\r\n",
        "    tag=nltk.pos_tag([word])[0][1][0].upper()\r\n",
        "    tag_dict={\"J\": wordnet.ADJ, \r\n",
        "              \"N\": wordnet.NOUN,\r\n",
        "              \"V\": wordnet.VERB,\r\n",
        "              \"R\": wordnet.ADV}\r\n",
        "    return tag_dict.get(tag,wordnet.NOUN)\r\n",
        "\r\n",
        "def lemma_stop(str):\r\n",
        "    lemmatizer = WordNetLemmatizer()\r\n",
        "    tokenizer = RegexpTokenizer('\\w+|\\$]\\d\\[+|\\S+,-')\r\n",
        "    tokenized = tokenizer.tokenize(str)\r\n",
        "    lemmatized = [lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in tokenized]\r\n",
        "    stop_words = set(stopwords.words('english'))\r\n",
        "    filtered_sentence = [w for w in lemmatized if w.lower() not in stop_words]\r\n",
        "    after_lemma_stop = ' '.join(w for w in filtered_sentence)\r\n",
        "    return filtered_sentence"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I822QChCQCvV",
        "outputId": "eaade0a9-cecf-46bb-b338-ee75280e0fbb"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/IR_domain_custom_dataset_processed.csv\")\r\n",
        "for column in data:\r\n",
        "    if data[column].isnull().any():\r\n",
        "        print('{0} has {1} null values'.format(column, data[column].isnull().sum()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text has 3 null values\n",
            "domain has 53555 null values\n",
            "word_count has 53558 null values\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSlOlJfAijkn"
      },
      "source": [
        "Droping Null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa2X9OjvQF7w"
      },
      "source": [
        "data = data.dropna()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss1XWb8kQN2j",
        "outputId": "9fdd8cad-cf3f-4cc3-a8bc-208561accb40"
      },
      "source": [
        "for column in data:\r\n",
        "    if data[column].isnull().any():\r\n",
        "        print('{0} has {1} null values'.format(column, data[column].isnull().sum()))\r\n",
        "data['word_count'].count()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99998"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "RVlEMbzzQUFv",
        "outputId": "721c831a-1cb4-4858-8624-a05dbb266037"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Text</th>\n",
              "      <th>domain</th>\n",
              "      <th>word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>buy new socceroos jersey think bit excessive f...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>43.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>england trip feel like dream assume lose germa...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>32.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>&lt;tag_1&gt; not rate henderson england player stop...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>awesome &lt;tag_1&gt; morning talk &lt;tag_2&gt; &lt;tag_3&gt; &amp;...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>46.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>utterly shocked hear leroy sane axe germany sq...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>45.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0 Unnamed: 0.1  ... domain  word_count\n",
              "0           0            1  ...    1.0        43.0\n",
              "1           1            6  ...    1.0        32.0\n",
              "2           2            7  ...    1.0        30.0\n",
              "3           3            9  ...    1.0        46.0\n",
              "4           4           13  ...    1.0        45.0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9ptQsG-QV8t",
        "outputId": "56b5d6b6-a283-46d1-eb74-13dff950a298"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 99998 entries, 0 to 153555\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Unnamed: 0    99998 non-null  int64  \n",
            " 1   Unnamed: 0.1  99998 non-null  object \n",
            " 2   Text          99998 non-null  object \n",
            " 3   domain        99998 non-null  float64\n",
            " 4   word_count    99998 non-null  float64\n",
            "dtypes: float64(2), int64(1), object(2)\n",
            "memory usage: 4.6+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRxzEaYpQYHO"
      },
      "source": [
        "data = np.array(data)\r\n",
        "np.save(\"datan\",data)\r\n",
        "data = np.load(\"datan.npy\",allow_pickle = True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g51HFSjCQbVw",
        "outputId": "9188617b-7d69-4761-9e6a-cbb5be988db1"
      },
      "source": [
        "data"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, '1',\n",
              "        'buy new socceroos jersey think bit excessive figure wear game game great value worldcuprussia2018 aus # socceroos',\n",
              "        1.0, 43.0],\n",
              "       [1, '6',\n",
              "        'england trip feel like dream assume lose germany quarter penalty thank goodness not happen cheer <tag_1>',\n",
              "        1.0, 32.0],\n",
              "       [2, '7',\n",
              "        '<tag_1> not rate henderson england player stop shit oh solid blue # comeonengland # coyb',\n",
              "        1.0, 30.0],\n",
              "       ...,\n",
              "       [153553, '98920',\n",
              "        '<tag_1> <tag_2> <tag_3> <tag_4> nerp not buy alias invalidate claim responsibility',\n",
              "        3.0, 16.0],\n",
              "       [153554, '98921',\n",
              "        '<tag_1> oh leaker ok hillary talk wiki leak fbi leak hypocrite',\n",
              "        3.0, 23.0],\n",
              "       [153555, '98922',\n",
              "        '<tag_1> need convince people think great yesterday disturb',\n",
              "        3.0, 14.0]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_Mnd-hHi9f1"
      },
      "source": [
        "Creating Doc ID and index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtSBY8BUQdrx"
      },
      "source": [
        "get_docID = {}\r\n",
        "get_index = {}\r\n",
        "\r\n",
        "\r\n",
        "NN = len(data)\r\n",
        "for i in range(0, len(data)) :\r\n",
        "    get_docID[i] = data[i][0]\r\n",
        "    get_index[data[i][0]] = i\r\n",
        "print(get_docID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze8pEyF4Q_YA"
      },
      "source": [
        "def is_not_credible (text):\r\n",
        "    match = re.search(r'[!@#?&{}()]', text)\r\n",
        "    \r\n",
        "    if match:\r\n",
        "        return True\r\n",
        "    else:\r\n",
        "        return False"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9XlFf34RDEX"
      },
      "source": [
        "def scrub_words(text):\r\n",
        "    text = re.sub('[!@#?&{}()]', '', text)\r\n",
        "    text=re.sub(r'[^\\x00-\\x7F]',\" \",text)\r\n",
        "    return text"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIp3Zs79RFdX"
      },
      "source": [
        "def clean_document (document_string):\r\n",
        "    cleaned_doc = document_string\r\n",
        "    for word in document_string.split():\r\n",
        "      if is_not_credible(word):\r\n",
        "        temp= scrub_words(word)\r\n",
        "        split=wordninja.split(temp)\r\n",
        "        if len(split)>7:\r\n",
        "              cleaned_doc = cleaned_doc.replace(word,'')\r\n",
        "    return cleaned_doc"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxEPwEMgRHxT",
        "outputId": "c9604e05-71cd-4cfc-a824-a1dcd5a24dca"
      },
      "source": [
        "subset = []\r\n",
        "counter = 0\r\n",
        "for document in data:\r\n",
        "    subset.append(document)\r\n",
        "    counter += 1\r\n",
        "    if counter == NN:\r\n",
        "        break\r\n",
        "print(len(subset))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm9oNY4MRKsv"
      },
      "source": [
        "from datetime import datetime\r\n",
        "def replace_dates(documentString):\r\n",
        "    \r\n",
        "    regEx = '(([0-9]+(/|\\\\.|-)[0-9]+(/|\\\\.|-)[0-9]+)|([0-9]+(/|\\\\.|-)[0-9]+))'\r\n",
        "    iterator = re.finditer(regEx, documentString)\r\n",
        "    listOfDates = [(m.start(0), m.end(0)) for m in iterator]\r\n",
        "    \r\n",
        "    for indices in listOfDates:\r\n",
        "        date = documentString[indices[0]:indices[1]]\r\n",
        "        tmp = date\r\n",
        "        date = date.replace('.', '/')\r\n",
        "        date = date.replace('-', '/')\r\n",
        "        count = date.count('/')\r\n",
        "        newDate = ''\r\n",
        "        if count == 2:\r\n",
        "            try:\r\n",
        "                newDate = datetime.strptime(date, '%m/%d/%Y').strftime('%d %b %Y')\r\n",
        "            except ValueError as ve:\r\n",
        "                newDate = date\r\n",
        "        else:\r\n",
        "            try:\r\n",
        "                newDate = datetime.strptime(date, '%m/%d').strftime('%d %b')\r\n",
        "            except ValueError as ve:\r\n",
        "                newDate = date\r\n",
        "                \r\n",
        "        newDate = newDate.replace(' ', '')\r\n",
        "        documentString = documentString.replace(tmp, newDate)\r\n",
        "        # print(newDate)\r\n",
        "    \r\n",
        "    return documentString"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6brfqTNRNEy",
        "outputId": "40606ce0-0dfe-4920-fc4e-64b11c0e47bf"
      },
      "source": [
        "import time\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "start = time.time()\r\n",
        "\r\n",
        "contents = []\r\n",
        "for document in tqdm(subset):\r\n",
        "    # actually modifying the document\r\n",
        "    document[2] = remove_htmlcodes(document[2])\r\n",
        "    \r\n",
        "    # not actually modifying the document\r\n",
        "    modifiedContent = replace_dates(document[2])\r\n",
        "    modifiedContent = lemma_stop(clean_document(modifiedContent))\r\n",
        "    \r\n",
        "    # case-folding\r\n",
        "    for i in range(len(modifiedContent)):\r\n",
        "        modifiedContent[i] = modifiedContent[i].lower()\r\n",
        "  \r\n",
        "    contents.append(modifiedContent)\r\n",
        "    \r\n",
        "print(time.time() - start)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 99998/99998 [05:52<00:00, 283.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "352.4477972984314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSf8JlksUzFg",
        "outputId": "08fca3ad-db7e-4a9e-a16c-16ae77fcc45d"
      },
      "source": [
        "pip install unidecode"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 25.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 32.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 35.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 30.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 32.7MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 35.4MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 27.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 23.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 25.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 23.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 23.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 23.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 23.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 23.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 23.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 23.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 23.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 23.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 23.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 23.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 23.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 23.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 23.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 23.4MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuLLNRdCRQIq"
      },
      "source": [
        "import unidecode\r\n",
        "contents_temp = contents\r\n",
        "\r\n",
        "\r\n",
        "for i in range(NN):\r\n",
        "    for j in range(len(contents[i])):\r\n",
        "        contents[i][j] = unidecode.unidecode(contents[i][j])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYpW7BJZlMg4"
      },
      "source": [
        "Trie "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nKR8_-sVfWH"
      },
      "source": [
        "class Node:\r\n",
        "    \r\n",
        "    def __init__(self):\r\n",
        "        \r\n",
        "        self.prefixes = 0\r\n",
        "        self.words = 0\r\n",
        "        # self.children = [None for i in range (128)]\r\n",
        "        self.children = {}\r\n",
        "        \r\n",
        "    def add(self, suffix, currentIndex):\r\n",
        "\r\n",
        "        '''Adds suffix[currenIndex : ] to self'''\r\n",
        "\r\n",
        "        if currentIndex == len(suffix):\r\n",
        "            self.words = self.words + 1\r\n",
        "            self.prefixes = self.prefixes + 1\r\n",
        "        \r\n",
        "        else:\r\n",
        "            self.prefixes = self.prefixes + 1\r\n",
        "            firstChar = suffix[currentIndex]\r\n",
        "            # if self.children[ord(firstChar)] == None:\r\n",
        "            #     newNode = Node()\r\n",
        "            #     self.children[ord(firstChar)] = newNode\r\n",
        "            if ord(firstChar) not in self.children:\r\n",
        "                newNode = Node()\r\n",
        "                self.children[ord(firstChar)] = newNode\r\n",
        "            currentIndex += 1\r\n",
        "            self.children[ord(firstChar)].add(suffix, currentIndex)\r\n",
        "                \r\n",
        "    \r\n",
        "    def count_words(self, suffix, currentIndex):\r\n",
        "\r\n",
        "        '''Returns the count of words ending with suffix[currentIndex]'''\r\n",
        "        \r\n",
        "        if currentIndex == len(suffix):\r\n",
        "            return self.words\r\n",
        "        \r\n",
        "        else:\r\n",
        "            firstChar = suffix[currentIndex]\r\n",
        "            # if self.children[ord(firstChar)] == None:\r\n",
        "            #     return 0\r\n",
        "            if ord(firstChar) not in self.children:\r\n",
        "                return 0\r\n",
        "            else:\r\n",
        "                currentIndex += 1\r\n",
        "                return self.children[ord(firstChar)].count_words(suffix, currentIndex)\r\n",
        "            \r\n",
        "    def count_prefixes(self, suffix, currentIndex):\r\n",
        "\r\n",
        "        '''Returns the count of prefixes ending with suffix[currentIndex]'''\r\n",
        "        \r\n",
        "        if currentIndex == len(suffix):\r\n",
        "            return self.prefixes\r\n",
        "        \r\n",
        "        else:\r\n",
        "            firstChar = suffix[currentIndex]\r\n",
        "            # if self.children[ord(firstChar)] == None:\r\n",
        "            #     return 0\r\n",
        "            if ord(firstChar) not in self.children:\r\n",
        "                return 0\r\n",
        "            else:\r\n",
        "                currentIndex += 1\r\n",
        "                return self.children[ord(firstChar)].count_prefixes(suffix, currentIndex)\r\n",
        "            \r\n",
        "class CollectionNode(Node):\r\n",
        "    \r\n",
        "    def __init__(self):\r\n",
        "        super(CollectionNode,self).__init__()\r\n",
        "        self.documents = set()\r\n",
        "        self.titles = set()\r\n",
        "\r\n",
        "    def add_title(self, suffix, currentIndex, docID):\r\n",
        "\r\n",
        "        '''Adds suffix[currenIndex : ] to self and appends docID to self.titles\r\n",
        "        at the end of suffix'''\r\n",
        "\r\n",
        "        if currentIndex == len(suffix):\r\n",
        "            self.words = self.words + 1\r\n",
        "            self.prefixes = self.prefixes + 1\r\n",
        "            self.titles.add(docID)\r\n",
        "\r\n",
        "        else:\r\n",
        "            self.prefixes = self.prefixes + 1\r\n",
        "            firstChar = suffix[currentIndex]\r\n",
        "            # if self.children[ord(firstChar)] == None:\r\n",
        "            #     newNode = CollectionNode()\r\n",
        "            #     self.children[ord(firstChar)] = newNode\r\n",
        "            if ord(firstChar) not in self.children:\r\n",
        "                newNode = CollectionNode()\r\n",
        "                self.children[ord(firstChar)] = newNode\r\n",
        "            currentIndex += 1\r\n",
        "            self.children[ord(firstChar)].add_title(suffix, currentIndex, docID)\r\n",
        "\r\n",
        "        \r\n",
        "    def add_document(self, suffix, currentIndex, docID):\r\n",
        "\r\n",
        "        '''Adds docID to self.documents at the end of suffix'''\r\n",
        "        \r\n",
        "        if currentIndex == len(suffix):\r\n",
        "            self.words = self.words + 1\r\n",
        "            self.prefixes = self.prefixes + 1\r\n",
        "            self.documents.add(docID)\r\n",
        "        \r\n",
        "        else:\r\n",
        "            self.prefixes = self.prefixes + 1\r\n",
        "            firstChar = suffix[currentIndex]\r\n",
        "            # if self.children[ord(firstChar)] == None:\r\n",
        "            #     newNode = CollectionNode()\r\n",
        "            #     self.children[ord(firstChar)] = newNode\r\n",
        "            if ord(firstChar) not in self.children:\r\n",
        "                newNode = CollectionNode()\r\n",
        "                self.children[ord(firstChar)] = newNode\r\n",
        "            currentIndex += 1\r\n",
        "            self.children[ord(firstChar)].add_document(suffix, currentIndex, docID)\r\n",
        "            \r\n",
        "    def get_doc_list(self, suffix, currentIndex):\r\n",
        "\r\n",
        "        '''Returns list of documents containing suffix'''\r\n",
        "        \r\n",
        "        if currentIndex == len(suffix):\r\n",
        "            return self.documents\r\n",
        "        \r\n",
        "        else:\r\n",
        "            firstChar = suffix[currentIndex]\r\n",
        "            # if self.children[ord(firstChar)] == None:\r\n",
        "            #     return []\r\n",
        "            if ord(firstChar) not in self.children:\r\n",
        "                return []\r\n",
        "            else:\r\n",
        "                currentIndex += 1\r\n",
        "                return self.children[ord(firstChar)].get_doc_list(suffix, currentIndex)\r\n",
        "\r\n",
        "    def get_title_list(self, suffix, currentIndex):\r\n",
        "\r\n",
        "        '''Returns list of titles that contain suffix'''\r\n",
        "        \r\n",
        "        if currentIndex == len(suffix):\r\n",
        "            return self.titles\r\n",
        "        \r\n",
        "        else:\r\n",
        "            firstChar = suffix[currentIndex]\r\n",
        "            # if self.children[ord(firstChar)] == None:\r\n",
        "            #     return []\r\n",
        "            if ord(firstChar) not in self.children:\r\n",
        "                return []\r\n",
        "            else:\r\n",
        "                currentIndex += 1\r\n",
        "                return self.children[ord(firstChar)].get_title_list(suffix, currentIndex)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2whedEblfXj"
      },
      "source": [
        "##### Create map from docID of the document to an object of class Node (i.e, the corresponding document trie structure)\r\n",
        "#####ex. if the docID of the document is 1, \r\n",
        "#####getReference[1] gives the object which is the trie structure of docID 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNVi314_U2hs"
      },
      "source": [
        "import pickle \r\n",
        "\r\n",
        "getReference = {}\r\n",
        "documentRoot = []\r\n",
        "collection = CollectionNode()\r\n",
        "\r\n",
        "for i in range(NN):\r\n",
        "    newDocument = Node()\r\n",
        "    documentRoot.append(newDocument)\r\n",
        "    getReference[get_docID[i]] = newDocument"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R_B3HvOVAbT",
        "outputId": "87df9e42-4d16-43fb-fe58-df28fbb2a307"
      },
      "source": [
        "max_tf = {}\r\n",
        "\r\n",
        "import time\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "start = time.time()\r\n",
        "for i in tqdm(range(NN)):\r\n",
        "    for w in contents_temp[i]:\r\n",
        "        collection.add_document(w, 0, get_docID[i])\r\n",
        "        documentRoot[i].add(w, 0)\r\n",
        "        if get_docID[i] in max_tf:\r\n",
        "            max_tf[get_docID[i]] = max(documentRoot[i].count_words(w, 0), max_tf[get_docID[i]])\r\n",
        "        else:\r\n",
        "            max_tf[get_docID[i]] = documentRoot[i].count_words(w, 0)\r\n",
        "    \r\n",
        "        \r\n",
        "print(time.time() - start)  "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 99998/99998 [00:54<00:00, 1826.12it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "54.76555824279785\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbQh81D9VwxZ",
        "outputId": "a809c853-9818-4c37-8e9d-31a9e4fb08a5"
      },
      "source": [
        "import math\r\n",
        "import queue\r\n",
        "\r\n",
        "documentLength = {}\r\n",
        "N = len(documentRoot)\r\n",
        "\r\n",
        "\r\n",
        "for i in tqdm(range(len(documentRoot))):\r\n",
        "    \r\n",
        "    docID = get_docID[i]\r\n",
        "    length = 0\r\n",
        "    document = documentRoot[i]\r\n",
        "    q = queue.Queue()\r\n",
        "    q.put([document, ''])\r\n",
        "    \r\n",
        "    while q.qsize() > 0:\r\n",
        "\r\n",
        "        current = q.get()\r\n",
        "        reference = current[0]\r\n",
        "        word = current[1]\r\n",
        "\r\n",
        "        if reference.words > 0:\r\n",
        "            df = len(collection.get_doc_list(word, 0))\r\n",
        "            idf = math.log10(N/df)\r\n",
        "            # print(word, reference.words, df)\r\n",
        "            length += (reference.words * idf) ** 2\r\n",
        "\r\n",
        "        for i in range(256):\r\n",
        "            if reference.children.get(i) is not None:\r\n",
        "                new_word = word + chr(i)\r\n",
        "                q.put([reference.children[i], new_word])\r\n",
        "\r\n",
        "    # print(length**0.5)\r\n",
        "    documentLength[docID] = length**0.5\r\n",
        "    \r\n",
        "    \r\n",
        "    \r\n",
        "pickle_out = open(\"collection.pickle\",\"wb\")\r\n",
        "pickle.dump(collection,pickle_out)\r\n",
        "pickle_out.close()\r\n",
        "pickle_out1 = open(\"documentRoot.pickle\",\"wb\")\r\n",
        "pickle.dump(getReference,pickle_out1)\r\n",
        "pickle_out1.close()\r\n",
        "pickle_out2 = open(\"max_tf.pickle\",\"wb\")\r\n",
        "pickle.dump(max_tf,pickle_out2)\r\n",
        "pickle_out2.close()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 99998/99998 [07:19<00:00, 227.54it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE5vS3jaWR6X",
        "outputId": "30a944bc-e946-4a1d-f9fe-de841311b379"
      },
      "source": [
        "import sys\r\n",
        "print(sys.getsizeof(documentRoot))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "824464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZlRwuKSZXtU",
        "outputId": "862bfe1d-05dc-4ea4-ba5d-ef53f531d59b"
      },
      "source": [
        "query = input()\r\n",
        "final_query = replace_dates(query)\r\n",
        "final_query = lemma_stop(final_query)\r\n",
        "for i in range(len(final_query)):\r\n",
        "    final_query[i] = unidecode.unidecode(final_query[i])\r\n",
        "    \r\n",
        "    # case-folding\r\n",
        "    final_query[i] = final_query[i].lower()\r\n",
        "print(final_query)\r\n",
        "print(len(final_query))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I have bought several of the Vitality canned \n",
            "['bought', 'several', 'vitality']\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIYskSpWZs0Y"
      },
      "source": [
        "tf_query = {}\r\n",
        "for w in final_query:\r\n",
        "    if w not in tf_query:\r\n",
        "        tf_query[w] = 1\r\n",
        "    else:\r\n",
        "        tf_query[w] += 1\r\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvW9XCERZ1Pu",
        "outputId": "5821556d-cfef-462e-9d2a-417bbb91c975"
      },
      "source": [
        "# scores[i] stores the dot product of the tf-idf score vectors of the query and document of docID i in the corpus\r\n",
        "scores = {}\r\n",
        "title_score = {}\r\n",
        "\r\n",
        "# N is the total number of documents in the corpus\r\n",
        "N = len(documentRoot)\r\n",
        "\r\n",
        "# wordsInDoc[i] is a sorted list of (word, score) tuples where\r\n",
        "# score is the tf-idf score for the (word, <ith doc>) pair\r\n",
        "wordsInDoc = {}\r\n",
        "\r\n",
        "factor = {}\r\n",
        "\r\n",
        "import math\r\n",
        "import bisect\r\n",
        "\r\n",
        "for query_term in tf_query:\r\n",
        "    \r\n",
        "    docs_having_query_term = collection.get_doc_list(query_term, 0)\r\n",
        "    print(docs_having_query_term)\r\n",
        "    df = len(docs_having_query_term)\r\n",
        "    idf = 0\r\n",
        "    \r\n",
        "    print('-------------------------------------')\r\n",
        "    print('Term in query = ', query_term)\r\n",
        "    # print('List of documents with this term=', docs_having_query_term)\r\n",
        "    print()\r\n",
        "    \r\n",
        "    if df == 0:\r\n",
        "        idf = 0\r\n",
        "    else:\r\n",
        "        idf = math.log10(N/df)\r\n",
        "        \r\n",
        "    docs_having_query_term_in_title = collection.get_title_list(query_term,0)\r\n",
        "    print(docs_having_query_term_in_title)\r\n",
        "    for docID in docs_having_query_term_in_title:\r\n",
        "        if docID in title_score:\r\n",
        "            title_score[docID] += idf\r\n",
        "        else:\r\n",
        "            title_score[docID] = idf\r\n",
        "        \r\n",
        "    print('df = ',df)\r\n",
        "    print('idf = ',idf)\r\n",
        "    \r\n",
        "    tfidf_query = tf_query[query_term] * idf\r\n",
        "        \r\n",
        "    for docID in docs_having_query_term:\r\n",
        "        # print(docID)\r\n",
        "        tf_doc = getReference[docID].count_words(query_term, 0)\r\n",
        "        tf_doc = 0.5 + 0.5*tf_doc/max_tf[docID]\r\n",
        "        # print('tf for doc',docID,'is',tf_doc)\r\n",
        "        # tfidf_doc_query = tf_doc * idf\r\n",
        "        # tfidf_doc = 1 + math.log10(tf_doc)\r\n",
        "        tfidf_doc = (tf_doc)\r\n",
        "        # tfidf_doc_query = (tf_doc)\r\n",
        "        \r\n",
        "        # print('tfidf for doc',doc,'is',tfidf_doc)\r\n",
        "        # print()\r\n",
        "        \r\n",
        "        if docID not in scores:\r\n",
        "            scores[docID] = (tfidf_query * tfidf_doc)\r\n",
        "            wordsInDoc[docID] = []\r\n",
        "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\r\n",
        "            factor[docID] = idf\r\n",
        "        else:\r\n",
        "            scores[docID] += (tfidf_query * tfidf_doc)\r\n",
        "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\r\n",
        "            factor[docID] += idf\r\n",
        "print(title_score)\r\n",
        "for docID in scores:\r\n",
        "    if documentLength[docID] != 0:\r\n",
        "#         scores[docID] = scores[docID]/ math.sqrt(documentLength[docID])\r\n",
        "        scores[docID] *= factor[docID]\r\n",
        "        if docID in title_score:\r\n",
        "            scores[docID] *= 1 + title_score[docID]\r\n",
        "\r\n",
        "\r\n",
        "sorted_scores = sorted(scores.items(), key = lambda kv : kv[1] , reverse = True)\r\n",
        "\r\n",
        "maxshow = min(10, len(scores))\r\n",
        "\r\n",
        "print('\\n\\n')\r\n",
        "print('============================================')\r\n",
        "\r\n",
        "for i in range(maxshow):\r\n",
        "    # print(i)\r\n",
        "    print()\r\n",
        "    docID = sorted_scores[i][0]\r\n",
        "    print('doc ID = ', docID)\r\n",
        "    cnt = 0\r\n",
        "    print('Keywords:')\r\n",
        "    print()\r\n",
        "    print(subset[get_index[sorted_scores[i][0]]][2])\r\n",
        "    print()\r\n",
        "    # if sorted_scores[i][0] not in title_score:\r\n",
        "    #     print('title score = ',0)\r\n",
        "    # else:\r\n",
        "    #     print('title score = ',title_score[sorted_scores[i][0]])\r\n",
        "    for j in range(len(wordsInDoc[docID])):\r\n",
        "        print(wordsInDoc[docID][j][1], wordsInDoc[docID][j][0], end = ' ')\r\n",
        "        print(getReference[docID].count_words(wordsInDoc[docID][j][1], 0))\r\n",
        "    print()\r\n",
        "    print()\r\n",
        "    count = 0\r\n",
        "    found = 0\r\n",
        "    words_before=queue.Queue()\r\n",
        "    at_start = 1\r\n",
        "    display = \"\"\r\n",
        "    for word in subset[get_index[docID]][2].split():\r\n",
        "            \r\n",
        "        check_with=replace_dates(word)\r\n",
        "        check_with = check_with.lower()\r\n",
        "        if len(lemma_stop(check_with)) > 0:\r\n",
        "            check_with=lemma_stop(check_with)[0]\r\n",
        "        else:\r\n",
        "            check_with=word\r\n",
        "        \r\n",
        "        if check_with == wordsInDoc[docID][0][1]:\r\n",
        "            found=1\r\n",
        "            \r\n",
        "        if found == 1:\r\n",
        "            display = display + word + \" \"\r\n",
        "            count += 1\r\n",
        "            if count == 50:\r\n",
        "                break\r\n",
        "        if found == 0:\r\n",
        "            words_before.put(word)\r\n",
        "            if words_before.qsize()>20:\r\n",
        "                remove=words_before.get()\r\n",
        "                at_start=0\r\n",
        "                \r\n",
        "    if not at_start:\r\n",
        "        print('...', end = ' ')\r\n",
        "    while words_before.qsize() > 0:\r\n",
        "        print(words_before.get(), end = ' ')\r\n",
        "    print(display, end = ' ')\r\n",
        "    print('...', end = ' ')\r\n",
        "    print('\\n')\r\n",
        "    print('tf-idf score=', sorted_scores[i][1])\r\n",
        "    print('\\n')\r\n",
        "    print('============================================')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{52680, 67786, 53002, 52807}\n",
            "-------------------------------------\n",
            "Term in query =  bought\n",
            "\n",
            "set()\n",
            "df =  4\n",
            "idf =  4.397931322695539\n",
            "{53179}\n",
            "-------------------------------------\n",
            "Term in query =  several\n",
            "\n",
            "set()\n",
            "df =  1\n",
            "idf =  4.999991314023502\n",
            "{42663, 47465, 42603, 13548, 64555, 64558, 58993, 61009, 151089, 33334, 37975, 37976, 64536, 64858}\n",
            "-------------------------------------\n",
            "Term in query =  vitality\n",
            "\n",
            "set()\n",
            "df =  14\n",
            "idf =  3.8538632783452638\n",
            "{}\n",
            "\n",
            "\n",
            "\n",
            "============================================\n",
            "\n",
            "doc ID =  67786\n",
            "Keywords:\n",
            "\n",
            "<tag_1> <tag_2> <tag_3> bought&paid fbi work new world order not american people nwo turn\n",
            "\n",
            "bought -4.397931322695539 1\n",
            "\n",
            "\n",
            "<tag_1> <tag_2> <tag_3> bought&paid fbi work new world order not american people nwo turn  ... \n",
            "\n",
            "tf-idf score= 19.34179991914654\n",
            "\n",
            "\n",
            "============================================\n",
            "\n",
            "doc ID =  53179\n",
            "Keywords:\n",
            "\n",
            "purchase dog food amazon shipment satisfied open shipment can severely dent -several can dent multiple place inside can damage not appear issue case damage shipment inspect bulge small opening concern contamination food less frustrating issue risk contamination difficulty try scrape food depth large number dent can < confusion > simply unacceptable case ship horrible condition look like time start pick dog food local pet supply store sure can not damage\n",
            "\n",
            "several -3.124994571264689 1\n",
            "\n",
            "\n",
            "purchase dog food amazon shipment satisfied open shipment can severely dent -several can dent multiple place inside can damage not appear issue case damage shipment inspect bulge small opening concern contamination food less frustrating issue risk contamination difficulty try scrape food depth large number dent can < confusion > simply unacceptable case ship horrible condition look like time start pick dog  ... \n",
            "\n",
            "tf-idf score= 15.624945712694043\n",
            "\n",
            "\n",
            "============================================\n",
            "\n",
            "doc ID =  151089\n",
            "Keywords:\n",
            "\n",
            "<tag_1> tyrant nag sickly bag short year vote vitality healthy <tag_2> https < skeptical annoyed undecided uneasy hesitant > fexcp6ptr1\n",
            "\n",
            "vitality -3.8538632783452638 1\n",
            "\n",
            "\n",
            "<tag_1> tyrant nag sickly bag short year vote vitality healthy <tag_2> https < skeptical annoyed undecided uneasy hesitant > fexcp6ptr1  ... \n",
            "\n",
            "tf-idf score= 14.852262168178104\n",
            "\n",
            "\n",
            "============================================\n",
            "\n",
            "doc ID =  52680\n",
            "Keywords:\n",
            "\n",
            "gizmo get try web site love little duck sweet potato treats.gizmo go nuts open box treat bought!once try start little spin jump want buy gizmo love dog\n",
            "\n",
            "bought -2.9319542151303595 1\n",
            "\n",
            "\n",
            "gizmo get try web site love little duck sweet potato treats.gizmo go nuts open box treat bought!once try start little spin jump want buy gizmo love dog  ... \n",
            "\n",
            "tf-idf score= 12.894533279431023\n",
            "\n",
            "\n",
            "============================================\n",
            "\n",
            "doc ID =  53002\n",
            "Keywords:\n",
            "\n",
            "dog love puzzle think lot fun 3rd dog puzzle bought(each different brand < confusion > easy far dog catch puzzle right away dog slow learner take work not need help figure treat right away\n",
            "\n",
            "bought -2.7487070766847124 1\n",
            "\n",
            "\n",
            "dog love puzzle think lot fun 3rd dog puzzle bought(each different brand < confusion > easy far dog catch puzzle right away dog slow learner take work not need help figure treat right away  ... \n",
            "\n",
            "tf-idf score= 12.088624949466587\n",
            "\n",
            "\n",
            "============================================\n",
            "\n",
            "doc ID =  52807\n",
            "Keywords:\n",
            "\n",
            "product belief well regular flour guess not isle grocery store carb/ calorie difference regular wheat flour not mention white refined highly process taste delicious atkin dukan eat experience experience experience turn realize truth product insane craving carb want come product junk carb carb regular store bought/ restaurant style/ vending machines/ cafeteria food/ etc not use product seriously slip power serve size carbquick cup favor spend money regular slice bread acne golore minimal acne wheat make break atkin give clear skin week product tiny zit face unacceptable consider expense product not worth pay carbquick know carb quickly spike insulin better almond flour not wheat sensitive people fda not want ruin wheat industry < confusion > buy real flour\n",
            "\n",
            "bought -2.565459938239065 1\n",
            "\n",
            "\n",
            "... dukan eat experience experience experience turn realize truth product insane craving carb want come product junk carb carb regular store bought/ restaurant style/ vending machines/ cafeteria food/ etc not use product seriously slip power serve size carbquick cup favor spend money regular slice bread acne golore minimal acne wheat make break atkin give clear skin week product tiny zit face unacceptable consider expense product not worth pay carbquick know carb  ... \n",
            "\n",
            "tf-idf score= 11.282716619502148\n",
            "\n",
            "\n",
            "============================================\n",
            "\n",
            "doc ID =  13548\n",
            "Keywords:\n",
            "\n",
            "england belgium time <tag_1> woman team training match today gear vitality hockey woman world cup thank <tag_2> game week <tag_3> good luck tonight https < skeptical annoyed undecided uneasy hesitant > qfoodicnz8\n",
            "\n",
            "vitality -2.890397458758948 1\n",
            "\n",
            "\n",
            "england belgium time <tag_1> woman team training match today gear vitality hockey woman world cup thank <tag_2> game week <tag_3> good luck tonight https < skeptical annoyed undecided uneasy hesitant > qfoodicnz8  ... \n",
            "\n",
            "tf-idf score= 11.139196626133579\n",
            "\n",
            "\n",
            "============================================\n",
            "\n",
            "doc ID =  37976\n",
            "Keywords:\n",
            "\n",
            "hi order product website want post review product okay packaging look nice neat not messy like pure simple maybe simple not ingredient info automatic fail say allergy warning contain soy fail not tell gum whiten tooth fight bad breath know word vitality mean active taste like gum\n",
            "\n",
            "vitality -2.890397458758948 1\n",
            "\n",
            "\n",
            "... ingredient info automatic fail say allergy warning contain soy fail not tell gum whiten tooth fight bad breath know word vitality mean active taste like gum  ... \n",
            "\n",
            "tf-idf score= 11.139196626133579\n",
            "\n",
            "\n",
            "============================================\n",
            "\n",
            "doc ID =  64536\n",
            "Keywords:\n",
            "\n",
            "dog love treat feel good give natural healthy plus glucosamine eliminate joint stiffness vitamin increase vitality buy treat dog love\n",
            "\n",
            "vitality -2.890397458758948 1\n",
            "\n",
            "\n",
            "dog love treat feel good give natural healthy plus glucosamine eliminate joint stiffness vitamin increase vitality buy treat dog love  ... \n",
            "\n",
            "tf-idf score= 11.139196626133579\n",
            "\n",
            "\n",
            "============================================\n",
            "\n",
            "doc ID =  42603\n",
            "Keywords:\n",
            "\n",
            "give treat array doggie english bulldogs boston terrier hound mix mutt breed overall consensus enthusiastic paw not dog turn pretty impressive no matter fussy dog limit dietary restriction treat not drive wild add goody improve health ya not wrong try variety heart health breathie vitality < confusion > lil address dog specific need love\n",
            "\n",
            "vitality -2.569242185563509 1\n",
            "\n",
            "\n",
            "... dog limit dietary restriction treat not drive wild add goody improve health ya not wrong try variety heart health breathie vitality < confusion > lil address dog specific need love  ... \n",
            "\n",
            "tf-idf score= 9.901508112118735\n",
            "\n",
            "\n",
            "============================================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}